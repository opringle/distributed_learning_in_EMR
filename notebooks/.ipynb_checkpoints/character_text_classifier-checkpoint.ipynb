{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Read in already preprocessed data & show it in the form of a pandas DF\n",
    "    - Nothing to do with MXNet and a waste of people's time when it's char level\n",
    "- Build a bucketing iterator\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in previously preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train_df = pd.read_pickle('../data/ag_news_char/train.pickle')\n",
    "# test_df = pd.read_pickle('../data/ag_news_char/test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a custom bucketing iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import random\n",
    "import numpy as np\n",
    "from mxnet.io import DataIter, DataBatch, DataDesc\n",
    "from mxnet import ndarray\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class BucketUtteranceIter(DataIter):\n",
    "    \"\"\"\n",
    "    This iterator can handle variable length feature arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, utterances, intents, batch_size, buckets, data_pad=-1, label_pad=-1, data_name='utterance',\n",
    "                 label_name='label', dtype='float32'):\n",
    "        \"\"\"\n",
    "        :param utterances: list of list of int\n",
    "        :param intents: list of int\n",
    "        \"\"\"\n",
    "        super(BucketUtteranceIter, self).__init__()\n",
    "        buckets.sort()\n",
    "\n",
    "        nslice = 0  # Keep track of how many utterances are sliced\n",
    "        self.utterances = [[] for _ in buckets]\n",
    "        self.intents = [[] for _ in buckets]\n",
    "        self.indices = [[] for _ in buckets]\n",
    "\n",
    "        for i, utt in enumerate(utterances):\n",
    "            # Find the index of the smallest bucket that is larger than the sentence length\n",
    "            buck_idx = bisect.bisect_left(buckets, len(utt))\n",
    "\n",
    "            # Slice utterances that are too long to the largest bucket size\n",
    "            if buck_idx == len(buckets):\n",
    "                buck_idx = buck_idx - 1\n",
    "                nslice += 1\n",
    "                utt = utt[:buckets[buck_idx]]\n",
    "\n",
    "            # Pad utterances that are too short for their bucket\n",
    "            buff = np.full((buckets[buck_idx]), data_pad, dtype=dtype)\n",
    "            buff[:len(utt)] = utt\n",
    "\n",
    "            # Add data/label to bucket\n",
    "            self.utterances[buck_idx].append(buff)\n",
    "            self.intents[buck_idx].append(intents[i])\n",
    "            self.indices[buck_idx].append(i)\n",
    "\n",
    "        # Convert to list of array\n",
    "        self.utterances = [np.asarray(i, dtype=dtype) for i in self.utterances]\n",
    "        self.intents = [np.asarray(i, dtype=dtype) for i in self.intents]\n",
    "        self.indices = [np.asarray(i, dtype=dtype) for i in self.indices]\n",
    "\n",
    "        print(\"Warning, {0} utterances sliced to largest bucket size.\".format(nslice)) if nslice > 0 else None\n",
    "        print(\"Utterances per bucket: {}\\nBucket sizes: {}\".format([arr.shape[0] for arr in self.utterances], buckets))\n",
    "\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.dtype = dtype\n",
    "        self.data_pad = data_pad\n",
    "        self.label_pad = label_pad\n",
    "        self.default_bucket_key = max(buckets)\n",
    "        self.layout = 'NT'\n",
    "\n",
    "        self.provide_data = [DataDesc(name=self.data_name,\n",
    "                                      shape=(self.batch_size, self.default_bucket_key),\n",
    "                                      layout=self.layout)]\n",
    "        self.provide_label = [DataDesc(name=self.label_name,\n",
    "                                       shape=(self.batch_size, ),\n",
    "                                       layout=self.layout)]\n",
    "\n",
    "        # create empty list to store batch index values\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the iterator to the beginning of the data.\n",
    "        \"\"\"\n",
    "        self.curr_idx = 0\n",
    "        # shuffle data in each bucket\n",
    "        random.shuffle(self.idx)\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.indices[i], self.utterances[i], self.intents[i] = shuffle(self.indices[i],\n",
    "                                                                           self.utterances[i],\n",
    "                                                                           self.intents[i])\n",
    "        self.ndindex = []\n",
    "        self.ndsent = []\n",
    "        self.ndlabel = []\n",
    "\n",
    "        # append the lists with an array\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.ndindex.append(ndarray.array(self.indices[i], dtype=self.dtype))\n",
    "            self.ndsent.append(ndarray.array(self.utterances[i], dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(self.intents[i], dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Returns the next batch of data.\n",
    "        \"\"\"\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        # i = batches index, j = starting record\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        indices = self.ndindex[i][j:j + self.batch_size]\n",
    "        utterances = self.ndsent[i][j:j + self.batch_size]\n",
    "        intents = self.ndlabel[i][j:j + self.batch_size]\n",
    "\n",
    "        return DataBatch([utterances],\n",
    "                         [intents],\n",
    "                         pad=0,\n",
    "                         index=indices,\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[DataDesc(name=self.data_name, shape=utterances.shape, layout=self.layout)],\n",
    "                         provide_label=[DataDesc(name=self.label_name, shape=intents.shape, layout=self.layout)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, 1 utterances sliced to largest bucket size.\n",
      "Utterances per bucket: [1, 2, 1]\n",
      "Bucket sizes: [2, 5, 8]\n",
      "\n",
      "Batch 0\n",
      "Data\n",
      " [\n",
      "[[ 1.  2.  3.  4.  5.]\n",
      " [ 1.  2.  3.  4. -1.]]\n",
      "<NDArray 2x5 @cpu(0)>] \n",
      "Label\n",
      " [\n",
      "[4. 1.]\n",
      "<NDArray 2 @cpu(0)>]\n",
      " Bucket Key\n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "utterances = [\n",
    "    [1,2,3,4], \n",
    "    [1,2,3,4,5,6,7,8,9,10,11], \n",
    "    [1,2],\n",
    "    [1,2,3,4,5]\n",
    "]\n",
    "\n",
    "intents = [1,2,3,4]\n",
    "batch_size=2\n",
    "\n",
    "iterator = BucketUtteranceIter(utterances, intents, batch_size, buckets=[2,5,8])\n",
    "\n",
    "for i, batch in enumerate(iterator):\n",
    "    print(\"\\nBatch {}\\nData\\n {} \\nLabel\\n {}\\n Bucket Key\\n {}\".format(i, batch.data, batch.label, batch.bucket_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "def BucketedTokenCnnModule(token_to_index, intent_to_index, num_embed, filters, num_filter, dropout, default_bucket_key,\n",
    "                           context, invalid_label, batch_norm, smooth_alpha):\n",
    "    \"\"\"\n",
    "    builds an mxnet bucketing module\n",
    "    \"\"\"\n",
    "    def sym_gen(seq_len):\n",
    "        \"\"\"\n",
    "        :param seq_len: bucket size\n",
    "        :return: symbol for neural network architecture\n",
    "        \"\"\"\n",
    "        def conv(data, num_filter, kernel=(1, 1), stride=(1, 1), pad=(0, 0), name=None, suffix=''):\n",
    "            conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, no_bias=True,\n",
    "                                      name='%s%s_conv2d' % (name, suffix))\n",
    "            bn = mx.sym.BatchNorm(data=conv, name='%s%s_batchnorm' % (name, suffix), fix_gamma=True)\n",
    "            act = mx.sym.Activation(data=bn, act_type='relu', name='%s%s_relu' % (name, suffix))\n",
    "            return act\n",
    "\n",
    "        def conv_block(data, num_filter, name):\n",
    "            conv1 = conv(data, kernel=(1, 3), num_filter=num_filter, pad=(0, 1), name='conv1'+str(name))\n",
    "            conv2 = conv(conv1, kernel=(1, 3), num_filter=num_filter, pad=(0, 1), name='conv2'+str(name))\n",
    "            return conv2\n",
    "\n",
    "        X_shape, Y_shape = iterator.provide_data[0][1], iterator.provide_label[0][1]\n",
    "\n",
    "        data = mx.sym.Variable(name=\"data\")\n",
    "        softmax_label = mx.sym.Variable(name=\"softmax_label\")\n",
    "        print(\"data_input: \", data.infer_shape(data=X_shape)[1][0])\n",
    "        print(\"label input: \", softmax_label.infer_shape(softmax_label=Y_shape)[1][0])\n",
    "\n",
    "        # Embed each character to 16 channels\n",
    "        embedded_data = mx.sym.Embedding(data, input_dim=len(preprocessor.char_to_index), output_dim=hyperparameters['char_embed'])\n",
    "        embedded_data = mx.sym.Reshape(mx.sym.transpose(embedded_data, axes=(0, 2, 1)), shape=(0, 0, 1, -1))\n",
    "        print(\"embedded output: \", embedded_data.infer_shape(data=X_shape)[1][0])\n",
    "\n",
    "        # Temporal Convolutional Layer (without activation)\n",
    "        temp_conv_1 = mx.sym.Convolution(embedded_data, kernel=(1, 3), num_filter=hyperparameters['temp_conv_filters'], pad=(0, 1))\n",
    "        print(\"temp conv output: \", temp_conv_1.infer_shape(data=X_shape)[1][0])\n",
    "\n",
    "        # Create convolutional blocks with pooling in-between\n",
    "        channels = (hyperparameters['temp_conv_filters'],\n",
    "                    2 * hyperparameters['temp_conv_filters'],\n",
    "                    4 * hyperparameters['temp_conv_filters'],\n",
    "                    8 * hyperparameters['temp_conv_filters'])\n",
    "\n",
    "        blocks = (hyperparameters['block1_blocks'],\n",
    "                    hyperparameters['block2_blocks'],\n",
    "                    hyperparameters['block3_blocks'],\n",
    "                    hyperparameters['block4_blocks'])\n",
    "\n",
    "        for i, block_size in enumerate(blocks):\n",
    "            print(\"section {} ({} blocks)\".format(i, block_size))\n",
    "            for j in list(range(block_size)):\n",
    "                if i == 0 and j == 0:\n",
    "                    # first block follows the first temp conv layer\n",
    "                    block = conv_block(temp_conv_1, num_filter=channels[i], name='block'+str(i)+'_'+str(j))\n",
    "                elif j == 0:\n",
    "                    # this block follows a pooling layer\n",
    "                    block = conv_block(pool, num_filter=channels[i], name='block' + str(i) + '_' + str(j))\n",
    "                else:\n",
    "                    # this block follows a previous block\n",
    "                    block = conv_block(block, num_filter=channels[i], name='block'+str(i)+'_'+str(j))\n",
    "                print('\\tblock'+str(i)+'_'+str(j), block.infer_shape(data=X_shape)[1][0])\n",
    "            if i != len(blocks)-1:\n",
    "                # pool after each block size, excluding final layer\n",
    "                pool = mx.sym.Pooling(block, kernel=(1, 3), stride=(1, 2), pad=(0, 1), pool_type='max')\n",
    "                print('\\tblock' + str(i) + '_p', pool.infer_shape(data=X_shape)[1][0])\n",
    "\n",
    "        pool_k = block.infer_shape(data=X_shape)[1][0][3]\n",
    "        print(\"{0} pool kernel size {1}, stride 1\".format(hyperparameters['pool_type'], pool_k))\n",
    "        block = mx.sym.flatten(mx.sym.Pooling(block, kernel=(1, pool_k), stride=(1, 1), pad=(0, 0), pool_type=hyperparameters['pool_type']))\n",
    "        print(\"flattened pooling output: {1}\".format(pool_k, block.infer_shape(data=X_shape)[1][0]))\n",
    "        block = mx.sym.Dropout(block, p=hyperparameters['dropout'])\n",
    "        print(\"dropout output: \", block.infer_shape(data=X_shape)[1][0])\n",
    "\n",
    "        output = mx.sym.FullyConnected(block, num_hidden=len(preprocessor.label_to_index), flatten=True, name='output')\n",
    "        sm = mx.sym.SoftmaxOutput(output, softmax_label, hyperparameters['smooth_alpha'])\n",
    "        print(\"softmax output: \", sm.infer_shape(data=X_shape)[1][0])\n",
    "\n",
    "        return sm, ('utterance',), ('intent',)\n",
    "\n",
    "    return mx.mod.BucketingModule(sym_gen=sym_gen, default_bucket_key=default_bucket_key, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on varying input sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
