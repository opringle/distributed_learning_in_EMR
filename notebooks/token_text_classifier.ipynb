{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                  Fears for T N pension after talks   \n",
       "1  The Race is On: Second Private Team Sets Launc...   \n",
       "2      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "\n",
       "                                         description     label  \n",
       "0  Unions representing workers at Turner   Newall...  Business  \n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...  Sci/Tech  \n",
       "2  AP - A company founded by a chemistry research...  Sci/Tech  \n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...  Sci/Tech  \n",
       "4  AP - Southern California's smog-fighting agenc...  Sci/Tech  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../data/ag_news/train.csv')\n",
    "test_df = pd.read_csv('../data/ag_news/test.csv')\n",
    "\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data for MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "import spacy\n",
    "import regex as re\n",
    "\n",
    "\n",
    "class TokenPreprocessor:\n",
    "    def __init__(self, spacy_model, unseen_token=-1, pad_char='<padded>',max_tokens=20, unseen_label=-1):\n",
    "        self.unseen_token=unseen_token\n",
    "        self.pad_char = pad_char\n",
    "        self.max_tokens = max_tokens\n",
    "        self.unseen_label = unseen_label      \n",
    "        self.nlp = spacy.load(spacy_model)\n",
    "\n",
    "    def split_utterance(self, utterance):\n",
    "        \"\"\"\n",
    "        :param utterance: string\n",
    "        :return: list of string\n",
    "        \"\"\"\n",
    "        doc = self.nlp(utterance)\n",
    "        return [token.text for token in doc]\n",
    "    \n",
    "    def pad_utterance(self, tokenized_utterance):\n",
    "        \"\"\"\n",
    "        :param utterance: list of string\n",
    "        :param length: desired list length\n",
    "        :return: padded/sliced list\n",
    "        \"\"\"\n",
    "        diff = len(tokenized_utterance) - self.max_tokens\n",
    "        if diff > 0:\n",
    "            return tokenized_utterance[:self.max_tokens]\n",
    "        else:\n",
    "            return tokenized_utterance + [self.pad_char] * -diff\n",
    "\n",
    "    def build_vocab(self, data, depth=1, max_vocab_size=None):\n",
    "        \"\"\"\n",
    "        :param data: list of data\n",
    "        :param depth: depth of data list\n",
    "        :param max_vocab_size:\n",
    "        :return: dict and list mapping data to indices\n",
    "        \"\"\"\n",
    "        if depth >1:\n",
    "            data = list(itertools.chain.from_iterable(data)) # Make list 1D\n",
    "        data_counts = Counter(data)  # Count occurrences of each word in the list\n",
    "\n",
    "        vocabulary_inv = [x[0] for x in data_counts.most_common(max_vocab_size)]\n",
    "        vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "        return vocabulary, vocabulary_inv\n",
    "\n",
    "    def fit(self, utterances, labels):\n",
    "        \"\"\"\n",
    "        :param utterances: list of raw utterances\n",
    "        :param labels: list of raw labels\n",
    "        \"\"\"\n",
    "        split_utterances = [self.split_utterance(utterance) for utterance in utterances]\n",
    "        padded_utterances = [self.pad_utterance(utterance) for utterance in split_utterances]\n",
    "        self.token_to_index, self.index_to_token = self.build_vocab(padded_utterances, depth=2)\n",
    "        self.intent_to_index, self.index_to_intent = self.build_vocab(labels, depth=1)\n",
    "\n",
    "    def transform_utterance(self, utterance):\n",
    "        \"\"\"\n",
    "        :param utterance: raw utterance string\n",
    "        :return: preprocessed utterance\n",
    "        \"\"\"\n",
    "        split_utterance = self.split_utterance(utterance)\n",
    "        padded_utterances = self.pad_utterance(split_utterance)\n",
    "        return [self.token_to_index.get(token, self.unseen_token) for token in padded_utterances]\n",
    "\n",
    "    def transform_label(self, label):\n",
    "        \"\"\"\n",
    "        :param label: raw intent label\n",
    "        :return: indexed intent label\n",
    "        \"\"\"\n",
    "        return self.intent_to_index.get(label, self.unseen_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TokenPreprocessor(spacy_model='en_core_web_sm')\n",
    "preprocessor.fit(train_df['description'].tolist(), train_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to index mappings:\t{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n",
      "\n",
      "First 10 token to index mappings:\n",
      "\n",
      "{'the': 0, ',': 1, '-': 2, 'a': 3, 'to': 4, 'of': 5, 'in': 6, 'and': 7, 'on': 8, ' ': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"Label to index mappings:\\t{}\\n\\nFirst 10 token to index mappings:\\n\\n{}\".\n",
    "      format(preprocessor.intent_to_index, \n",
    "             {k: preprocessor.token_to_index[k] for k in list(preprocessor.token_to_index)[:10]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news looks bad today. ==> [14, 243, 1227, 1467, 79, 10, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "MXNet is awesome. No really... ==>[-1, 20, 34443, 10, 209, 2117, 536, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n"
     ]
    }
   ],
   "source": [
    "print(\"The news looks bad today. ==> {}\".format(preprocessor.transform_utterance(\"The news looks bad today.\")))\n",
    "print(\"MXNet is awesome. No really... ==>{}\".format(preprocessor.transform_utterance(\"MXNet is awesome. No really...\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array([preprocessor.transform_utterance(utt) for utt in train_df['description'].tolist()])\n",
    "Y_train = np.array([preprocessor.transform_label(label) for label in train_df['label'].tolist()])\n",
    "\n",
    "X_test = np.array([preprocessor.transform_utterance(utt) for utt in test_df['description'].tolist()])\n",
    "Y_test = np.array([preprocessor.transform_label(label) for label in test_df['label'].tolist()])\n",
    "\n",
    "batch_n=120\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(data=X_train, label=Y_train, batch_size=batch_n, shuffle=True)\n",
    "test_iter = mx.io.NDArrayIter(data=X_test, label=Y_test, batch_size=batch_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0\n",
      "X:\n",
      "[\n",
      "[[2.9700e+02 2.8400e+03 2.1690e+03 ... 1.0300e+03 1.0000e+01 1.5000e+01]\n",
      " [3.6900e+02 6.0000e+00 1.6900e+02 ... 2.9840e+03 7.6000e+01 5.6500e+02]\n",
      " [6.1040e+03 3.1910e+03 1.0000e+00 ... 2.1970e+03 2.2000e+01 2.2690e+03]\n",
      " ...\n",
      " [3.9300e+02 1.0000e+00 9.1000e+01 ... 9.0000e+00 1.3162e+04 0.0000e+00]\n",
      " [2.1000e+01 2.0000e+00 1.8100e+02 ... 1.0000e+00 5.0122e+04 7.6380e+03]\n",
      " [3.6900e+02 5.0000e+00 0.0000e+00 ... 0.0000e+00 1.7240e+03 3.5840e+03]]\n",
      "<NDArray 120x20 @cpu(0)>]\n",
      " Y:\n",
      "[\n",
      "[3. 1. 1. 0. 2. 3. 0. 0. 2. 2. 3. 1. 2. 2. 3. 1. 0. 1. 1. 2. 2. 3. 2. 1.\n",
      " 1. 0. 3. 1. 3. 3. 3. 1. 2. 1. 3. 1. 0. 2. 0. 3. 3. 2. 2. 3. 3. 3. 0. 0.\n",
      " 2. 2. 1. 2. 0. 2. 0. 2. 2. 1. 1. 0. 2. 2. 3. 1. 3. 0. 3. 1. 3. 2. 1. 3.\n",
      " 0. 2. 2. 0. 2. 1. 1. 1. 0. 0. 3. 0. 0. 3. 0. 1. 3. 1. 3. 1. 1. 0. 2. 2.\n",
      " 0. 0. 2. 2. 2. 0. 0. 3. 0. 1. 3. 1. 2. 1. 0. 0. 0. 1. 1. 1. 0. 3. 3. 3.]\n",
      "<NDArray 120 @cpu(0)>]\n"
     ]
    }
   ],
   "source": [
    "train_iter.reset()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i < 1:\n",
    "        print(\"\\nBatch {}\\nX:\\n{}\\n Y:\\n{}\".format(i, batch.data, batch.label))\n",
    "train_iter.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_gen(sentence_size, num_embed, vocab_size, num_label, filter_list, num_filter, dropout):\n",
    "    \n",
    "    input_x = mx.sym.Variable('data')\n",
    "    input_y = mx.sym.Variable('softmax_label')\n",
    "    \n",
    "    X_shape = (120,sentence_size)\n",
    "\n",
    "    # embedding layer\n",
    "    embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed)\n",
    "    conv_input = mx.sym.reshape(data=embed_layer, shape=(0, 1, sentence_size, num_embed))\n",
    "\n",
    "    # create convolution + (max) pooling layer for each filter operation\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_list):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "        relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1,1))\n",
    "        pooled_outputs.append(pooli)\n",
    "\n",
    "    # combine all pooled outputs\n",
    "    concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "    h_pool = mx.sym.reshape(data=concat, shape=(0, -1))\n",
    "    \n",
    "    # dropout layer\n",
    "    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "\n",
    "    fc = mx.sym.FullyConnected(data=h_drop, num_hidden=num_label)\n",
    "\n",
    "    # softmax output\n",
    "    sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "    print(\"Shapes assuming batch size 120:\\n\")\n",
    "    print(\"Data input shape: {}\".format(input_x.infer_shape(data=X_shape)[1][0]))\n",
    "    print(\"Embed output shape: {}\".format(embed_layer.infer_shape(data=X_shape)[1][0]))\n",
    "    print(\"Convolutional input shape: {}\".format(conv_input.infer_shape(data=X_shape)[1][0]))\n",
    "    print(\"Pooled output shape: {}\".format(concat.infer_shape(data=X_shape)[1][0]))\n",
    "    print(\"Reshaped pooled output shape: {}\".format(h_pool.infer_shape(data=X_shape)[1][0]))\n",
    "    print(\"Output layer shape: {}\".format(fc.infer_shape(data=X_shape)[1][0]))\n",
    "    \n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes assuming batch size 120:\n",
      "\n",
      "Data input shape: (120, 20)\n",
      "Embed output shape: (120, 20, 16)\n",
      "Convolutional input shape: (120, 1, 20, 16)\n",
      "Pooled output shape: (120, 300, 1, 1)\n",
      "Reshaped pooled output shape: (120, 300)\n",
      "Output layer shape: (120, 4)\n"
     ]
    }
   ],
   "source": [
    "symbol = sym_gen(sentence_size=20, \n",
    "                 num_embed=16, \n",
    "                 vocab_size=len(preprocessor.token_to_index), \n",
    "                 num_label=len(preprocessor.intent_to_index),\n",
    "                 filter_list=[3, 4, 5], \n",
    "                 num_filter=100, \n",
    "                 dropout=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "- State of the art test accuracy ~ 92%\n",
    "- Within 5 epochs, training on a cpu, discarding any tokens above 20 we come within ~4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = mx.mod.Module(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Train-accuracy=0.614978\n",
      "INFO:root:Epoch[0] Time cost=11.109\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.862240\n",
      "INFO:root:Epoch[1] Train-accuracy=0.872283\n",
      "INFO:root:Epoch[1] Time cost=31.858\n",
      "INFO:root:Epoch[1] Validation-accuracy=0.889193\n",
      "INFO:root:Epoch[2] Train-accuracy=0.906575\n",
      "INFO:root:Epoch[2] Time cost=37.516\n",
      "INFO:root:Epoch[2] Validation-accuracy=0.888411\n",
      "INFO:root:Epoch[3] Train-accuracy=0.925650\n",
      "INFO:root:Epoch[3] Time cost=39.815\n",
      "INFO:root:Epoch[3] Validation-accuracy=0.887240\n",
      "INFO:root:Epoch[4] Train-accuracy=0.938583\n",
      "INFO:root:Epoch[4] Time cost=39.530\n",
      "INFO:root:Epoch[4] Validation-accuracy=0.881380\n"
     ]
    }
   ],
   "source": [
    "module.fit(train_data=train_iter,\n",
    "           eval_data=test_iter,\n",
    "           eval_metric=mx.metric.Accuracy(),\n",
    "           optimizer='Adam',\n",
    "           optimizer_params={'learning_rate': 0.001},\n",
    "           initializer=mx.initializer.Uniform(0.1),\n",
    "           num_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(utterance, preprocessor, module):\n",
    "    \"\"\"\n",
    "    :param module: trained mxnet module\n",
    "    :param preprocessor: fit preprocessor\n",
    "    :param utterance: raw string for prediction\n",
    "    \"\"\"\n",
    "    preprocessed_utterance = preprocessor.transform_utterance(utterance)\n",
    "    numpy_utterance = np.array([preprocessed_utterance])\n",
    "    pred_iter = mx.io.NDArrayIter(data=numpy_utterance, label=np.array([0]), batch_size=1)\n",
    "    predicted_probabilities = module.predict(pred_iter).asnumpy().tolist()[0]\n",
    "    class_preds = [(preprocessor.index_to_intent[i], v) for i, v in enumerate(predicted_probabilities)]\n",
    "    return class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Business', 0.5599420666694641),\n",
       " ('Sci/Tech', 0.29694634675979614),\n",
       " ('Sports', 0.014404438436031342),\n",
       " ('World', 0.1287071704864502)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_preds = predict(\"Elon Musk wants to take Tesla private at $420 per share.\", preprocessor, module)\n",
    "class_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Modal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Business', 0.5599420666694641)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "max(class_preds,key=itemgetter(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
