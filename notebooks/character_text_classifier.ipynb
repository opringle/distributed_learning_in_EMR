{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Read in already preprocessed data & show it in the form of a pandas DF\n",
    "    - Nothing to do with MXNet and a waste of people's time when it's char level\n",
    "- Build a bucketing iterator\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in previously preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train_df = pd.read_pickle('../data/ag_news_char/train.pickle')\n",
    "# test_df = pd.read_pickle('../data/ag_news_char/test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a custom bucketing iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import random\n",
    "import numpy as np\n",
    "from mxnet.io import DataIter, DataBatch, DataDesc\n",
    "from mxnet import ndarray\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class BucketUtteranceIter(DataIter):\n",
    "    \"\"\"\n",
    "    This iterator can handle variable length feature arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, utterances, intents, batch_size, buckets, data_pad=-1, label_pad=-1, data_name='utterance',\n",
    "                 label_name='intent', dtype='float32'):\n",
    "        \"\"\"\n",
    "        :param utterances: list of list of int\n",
    "        :param intents: list of int\n",
    "        \"\"\"\n",
    "        super(BucketUtteranceIter, self).__init__()\n",
    "        buckets.sort()\n",
    "\n",
    "        nslice = 0  # Keep track of how many utterances are sliced\n",
    "        self.utterances = [[] for _ in buckets]\n",
    "        self.intents = [[] for _ in buckets]\n",
    "        self.indices = [[] for _ in buckets]\n",
    "\n",
    "        for i, utt in enumerate(utterances):\n",
    "            # Find the index of the smallest bucket that is larger than the sentence length\n",
    "            buck_idx = bisect.bisect_left(buckets, len(utt))\n",
    "\n",
    "            # Slice utterances that are too long to the largest bucket size\n",
    "            if buck_idx == len(buckets):\n",
    "                buck_idx = buck_idx - 1\n",
    "                nslice += 1\n",
    "                utt = utt[:buckets[buck_idx]]\n",
    "\n",
    "            # Pad utterances that are too short for their bucket\n",
    "            buff = np.full((buckets[buck_idx]), data_pad, dtype=dtype)\n",
    "            buff[:len(utt)] = utt\n",
    "\n",
    "            # Add data/label to bucket\n",
    "            self.utterances[buck_idx].append(buff)\n",
    "            self.intents[buck_idx].append(intents[i])\n",
    "            self.indices[buck_idx].append(i)\n",
    "\n",
    "        # Convert to list of array\n",
    "        self.utterances = [np.asarray(i, dtype=dtype) for i in self.utterances]\n",
    "        self.intents = [np.asarray(i, dtype=dtype) for i in self.intents]\n",
    "        self.indices = [np.asarray(i, dtype=dtype) for i in self.indices]\n",
    "\n",
    "        print(\"\\nWarning, {0} utterances sliced to largest bucket size.\".format(nslice)) if nslice > 0 else None\n",
    "        print(\"Utterances per bucket: {}\\nBucket sizes: {}\".format([arr.shape[0] for arr in self.utterances], buckets))\n",
    "\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.dtype = dtype\n",
    "        self.data_pad = data_pad\n",
    "        self.label_pad = label_pad\n",
    "        self.default_bucket_key = max(buckets)\n",
    "        self.layout = 'NT'\n",
    "\n",
    "        self.provide_data = [DataDesc(name=self.data_name,\n",
    "                                      shape=(self.batch_size, self.default_bucket_key),\n",
    "                                      layout=self.layout)]\n",
    "        self.provide_label = [DataDesc(name=self.label_name,\n",
    "                                       shape=(self.batch_size, ),\n",
    "                                       layout=self.layout)]\n",
    "\n",
    "        # create empty list to store batch index values\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the iterator to the beginning of the data.\n",
    "        \"\"\"\n",
    "        self.curr_idx = 0\n",
    "        # shuffle data in each bucket\n",
    "        random.shuffle(self.idx)\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.indices[i], self.utterances[i], self.intents[i] = shuffle(self.indices[i],\n",
    "                                                                           self.utterances[i],\n",
    "                                                                           self.intents[i])\n",
    "        self.ndindex = []\n",
    "        self.ndsent = []\n",
    "        self.ndlabel = []\n",
    "\n",
    "        # append the lists with an array\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.ndindex.append(ndarray.array(self.indices[i], dtype=self.dtype))\n",
    "            self.ndsent.append(ndarray.array(self.utterances[i], dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(self.intents[i], dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Returns the next batch of data.\n",
    "        \"\"\"\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        # i = batches index, j = starting record\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        indices = self.ndindex[i][j:j + self.batch_size]\n",
    "        utterances = self.ndsent[i][j:j + self.batch_size]\n",
    "        intents = self.ndlabel[i][j:j + self.batch_size]\n",
    "\n",
    "        return DataBatch([utterances],\n",
    "                         [intents],\n",
    "                         pad=0,\n",
    "                         index=indices,\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[DataDesc(name=self.data_name, shape=utterances.shape, layout=self.layout)],\n",
    "                         provide_label=[DataDesc(name=self.label_name, shape=intents.shape, layout=self.layout)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning, 507 utterances sliced to largest bucket size.\n",
      "Utterances per bucket: [34, 33, 66, 113, 754]\n",
      "Bucket sizes: [32, 64, 128, 256, 512]\n",
      "\n",
      "Warning, 507 utterances sliced to largest bucket size.\n",
      "Utterances per bucket: [34, 33, 66, 113, 754]\n",
      "Bucket sizes: [32, 64, 128, 256, 512]\n",
      "\n",
      "Batch 0 Bucket size 256\n",
      "Data\n",
      " [\n",
      "[[558. 949. 264. ...  -1.  -1.  -1.]\n",
      " [884.  96. 637. ...  -1.  -1.  -1.]\n",
      " [762. 302. 804. ...  -1.  -1.  -1.]\n",
      " ...\n",
      " [500. 258. 707. ...  -1.  -1.  -1.]\n",
      " [910. 374. 682. ...  -1.  -1.  -1.]\n",
      " [ 90. 623. 912. ...  -1.  -1.  -1.]]\n",
      "<NDArray 12x256 @cpu(0)>] \n",
      "Label\n",
      " [\n",
      "[1. 1. 2. 3. 2. 1. 3. 0. 1. 3. 0. 0.]\n",
      "<NDArray 12 @cpu(0)>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "utterances = [np.random.choice(range(1, 1000), size=random.randint(1, 1024), replace=True).tolist() for i in range(1000)]\n",
    "intents = [random.randint(0, 3) for i in range(1000)]\n",
    "\n",
    "batch_size=12\n",
    "\n",
    "train_iter = BucketUtteranceIter(utterances, intents, batch_size, buckets=[32,64,128,256,512])\n",
    "test_iter = BucketUtteranceIter(utterances, intents, batch_size, buckets=[32,64,128,256,512])\n",
    "\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i < 1:\n",
    "        print(\"\\nBatch {} Bucket size {}\\nData\\n {} \\nLabel\\n {}\\n\".format(i, batch.bucket_key, batch.data, batch.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network symbol/module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "\n",
    "def bucketed_module(train_iter, vocab_size, dropout,num_label, smooth_alpha, default_bucket_key, context):\n",
    "    \"\"\"\n",
    "    :param train_iter:\n",
    "    :param vocab_size:\n",
    "    :param dropout:\n",
    "    :param num_label:\n",
    "    :param smooth_alpha:\n",
    "    :param default_bucket_key:\n",
    "    :param context:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def sym_gen(seq_len):\n",
    "        \"\"\"\n",
    "        :param seq_len: bucket size\n",
    "        :return: symbol for neural network architecture\n",
    "        \"\"\"\n",
    "        def conv(data, num_filter, kernel=(1, 1), stride=(1, 1), pad=(0, 0), name=None, suffix=''):\n",
    "            conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad,\n",
    "                                      no_bias=True, name='%s%s_conv2d' % (name, suffix))\n",
    "            bn = mx.sym.BatchNorm(data=conv, name='%s%s_batchnorm' % (name, suffix), fix_gamma=True)\n",
    "            act = mx.sym.Activation(data=bn, act_type='relu', name='%s%s_relu' % (name, suffix))\n",
    "            return act\n",
    "\n",
    "        def conv_block(data, num_filter, name):\n",
    "            conv1 = conv(data, kernel=(1, 3), num_filter=num_filter, pad=(0, 1), name='conv1' + str(name))\n",
    "            conv2 = conv(conv1, kernel=(1, 3), num_filter=num_filter, pad=(0, 1), name='conv2' + str(name))\n",
    "            return conv2\n",
    "\n",
    "        X_shape = (train_iter.batch_size, seq_len)\n",
    "        Y_shape = (train_iter.batch_size, )\n",
    "        \n",
    "        print(\"\\nNetwork architecture for bucket size {}\\n\".format(seq_len))\n",
    "\n",
    "        data = mx.sym.Variable(name=\"utterance\")\n",
    "        softmax_label = mx.sym.Variable(name=\"intent\")\n",
    "        print(\"data_input: \", data.infer_shape(utterance=X_shape)[1][0])\n",
    "        print(\"label input: \", softmax_label.infer_shape(intent=Y_shape)[1][0])\n",
    "\n",
    "        # Embed each character to 16 channels\n",
    "        embedded_data = mx.sym.Embedding(data, input_dim=vocab_size, output_dim=16)\n",
    "        embedded_data = mx.sym.Reshape(mx.sym.transpose(embedded_data, axes=(0, 2, 1)), shape=(0, 0, 1, -1))\n",
    "        print(\"embed layer output shape: \", embedded_data.infer_shape(utterance=X_shape)[1][0])\n",
    "\n",
    "        # Temporal Convolutional Layer (without activation)\n",
    "        temp_conv = mx.sym.Convolution(embedded_data, kernel=(1, 3), num_filter=64, pad=(0, 1))\n",
    "        print(\"Temp conv output shape: \", temp_conv.infer_shape(utterance=X_shape)[1][0])\n",
    "\n",
    "        # Create convolutional blocks with pooling in-between\n",
    "        block = conv_block(temp_conv, num_filter=64, name='block1_1')\n",
    "        block = conv_block(block, num_filter=64, name='block1_2')\n",
    "        pool = mx.sym.Pooling(block, kernel=(1, 3), stride=(1, 2), pad=(0, 1), pool_type='max')\n",
    "        print(\"Block 1 output shape: {}\".format(pool.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        block = conv_block(pool, num_filter=128, name='block2_1')\n",
    "        block = conv_block(block, num_filter=128, name='block2_2')\n",
    "        pool = mx.sym.Pooling(block, kernel=(1, 3), stride=(1, 2), pad=(0, 1), pool_type='max')\n",
    "        print(\"Block 2 output shape: {}\".format(pool.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        block = conv_block(pool, num_filter=256, name='block3_1')\n",
    "        block = conv_block(block, num_filter=256, name='block3_2')\n",
    "        pool = mx.sym.Pooling(block, kernel=(1, 3), stride=(1, 2), pad=(0, 1), pool_type='max')\n",
    "        print(\"Block 3 output shape: {}\".format(pool.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        block = conv_block(pool, num_filter=512, name='block4_1')\n",
    "        block = conv_block(block, num_filter=512, name='block4_2')\n",
    "        print(\"Block 4 output shape: {}\".format(block.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        pool_k = seq_len // 8\n",
    "        print(\"{0} pool kernel size {1}, stride 1\".format('avg', pool_k))\n",
    "        block = mx.sym.flatten(mx.sym.Pooling(block, kernel=(1, pool_k), stride=(1, 1), pad=(0, 0), pool_type='avg'))\n",
    "        print(\"flattened pooling output shape: {}\".format(block.infer_shape(utterance=X_shape)[1][0]))\n",
    "        block = mx.sym.Dropout(block, p=dropout)\n",
    "        print(\"dropout layer output shape: {}\".format(block.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        output = mx.sym.FullyConnected(block, num_hidden=num_label, flatten=True, name='output')\n",
    "        sm = mx.sym.SoftmaxOutput(output, softmax_label, smooth_alpha)\n",
    "        print(\"softmax output shape: {}\".format(sm.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        return sm, ('utterance',), ('intent',)\n",
    "\n",
    "    return mx.mod.BucketingModule(sym_gen=sym_gen, default_bucket_key=default_bucket_key, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture for bucket size 512\n",
      "\n",
      "data_input:  (12, 512)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 512)\n",
      "Temp conv output shape:  (12, 64, 1, 512)\n",
      "Block 1 output shape: (12, 64, 1, 256)\n",
      "Block 2 output shape: (12, 128, 1, 128)\n",
      "Block 3 output shape: (12, 256, 1, 64)\n",
      "Block 4 output shape: (12, 512, 1, 64)\n",
      "avg pool kernel size 64, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n"
     ]
    }
   ],
   "source": [
    "module = bucketed_module(train_iter, \n",
    "                         vocab_size=150, \n",
    "                         dropout=0.02,\n",
    "                         num_label=4,\n",
    "                         smooth_alpha=0.004,\n",
    "                         default_bucket_key=train_iter.default_bucket_key, \n",
    "                         context=mx.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on varying input sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture for bucket size 512\n",
      "\n",
      "data_input:  (12, 512)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 512)\n",
      "Temp conv output shape:  (12, 64, 1, 512)\n",
      "Block 1 output shape: (12, 64, 1, 256)\n",
      "Block 2 output shape: (12, 128, 1, 128)\n",
      "Block 3 output shape: (12, 256, 1, 64)\n",
      "Block 4 output shape: (12, 512, 1, 64)\n",
      "avg pool kernel size 64, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-1b73589c3b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                              },\n\u001b[1;32m     20\u001b[0m            \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m            num_epoch=30)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/text_classification/lib/python3.6/site-packages/mxnet/module/base_module.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, eval_data, eval_metric, epoch_end_callback, batch_end_callback, kvstore, optimizer, optimizer_params, eval_end_callback, eval_batch_end_callback, initializer, arg_params, aux_params, allow_missing, force_rebind, force_init, begin_epoch, num_epoch, validation_metric, monitor, sparse_row_id_fn)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mend_of_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mnext_data_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend_of_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_data_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/text_classification/lib/python3.6/site-packages/mxnet/io.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miter_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-4f6a382b5af5>\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;31m# i = batches index, j = starting record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reduce learning rate every 3 epochs\n",
    "batches_per_epoch = int(sum([len(bucket) for bucket in train_iter.ndsent])/batch_size)\n",
    "step = 3 * batches_per_epoch\n",
    "schedule = mx.lr_scheduler.FactorScheduler(step=step, factor=0.97)\n",
    "\n",
    "# Initialize convolutional filter weights using MSRAPRelu to aid training deeper architectures\n",
    "init = mx.initializer.Mixed(patterns=['conv2d_weight', '.*'],\n",
    "                            initializers=[mx.initializer.MSRAPrelu(factor_type='avg', slope=0.25),\n",
    "                                          mx.initializer.Normal(sigma=0.02)])\n",
    "\n",
    "# Learn network weights from data\n",
    "module.fit(train_data=train_iter,\n",
    "           eval_data=test_iter,\n",
    "           eval_metric=mx.metric.Accuracy(),\n",
    "           optimizer='sgd',\n",
    "           optimizer_params={'learning_rate': 0.06,\n",
    "                             'momentum': 0.93,\n",
    "                             'lr_scheduler': schedule,\n",
    "                             },\n",
    "           initializer=init,\n",
    "           num_epoch=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
