{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Read in already preprocessed data & show it in the form of a pandas DF\n",
    "    - Nothing to do with MXNet and a waste of people's time when it's char level\n",
    "    - Read in dictionaries also\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in previously preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dictionary mapping characters to integer indices:\n",
      "{' ': 0, 'e': 1, 'a': 2, 't': 3, 'i': 4, 'n': 5, 'o': 6, 's': 7, 'r': 8, 'l': 9, 'h': 10, 'd': 11, 'c': 12, 'u': 13, 'm': 14, 'p': 15, 'f': 16, 'g': 17, 'y': 18, 'w': 19, 'b': 20, '.': 21, 'v': 22, 'k': 23, ',': 24, '-': 25, ';': 26, '0': 27, '3': 28, 'x': 29, 'q': 30, 'j': 31, '9': 32, '1': 33, '#': 34, '2': 35, '\\\\': 36, '(': 37, ')': 38, \"'\": 39, 'z': 40, '/': 41, '5': 42, '&': 43, '4': 44, '6': 45, '\"': 46, ':': 47, '7': 48, '8': 49, '=': 50, '$': 51, '?': 52, '!': 53, '_': 54, '*': 55}\n",
      "\n",
      "Dictionary mapping labels to integer indices:\n",
      "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>Business</td>\n",
       "      <td>[8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 7, 10, 6, 8, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Business</td>\n",
       "      <td>[8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 15, 8, 4, 22,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Business</td>\n",
       "      <td>[8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 7, 6, 2, 8, 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Business</td>\n",
       "      <td>[8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 2, 13, 3, 10,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Business</td>\n",
       "      <td>[2, 16, 15, 0, 25, 0, 3, 1, 2, 8, 2, 19, 2, 18...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description     label  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  Business   \n",
       "1  Reuters - Private investment firm Carlyle Grou...  Business   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  Business   \n",
       "3  Reuters - Authorities have halted oil export\\f...  Business   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  Business   \n",
       "\n",
       "                                                   X  Y  \n",
       "0  [8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 7, 10, 6, 8, ...  0  \n",
       "1  [8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 15, 8, 4, 22,...  0  \n",
       "2  [8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 7, 6, 2, 8, 4...  0  \n",
       "3  [8, 1, 13, 3, 1, 8, 7, 0, 25, 0, 2, 13, 3, 10,...  0  \n",
       "4  [2, 16, 15, 0, 25, 0, 3, 1, 2, 8, 2, 19, 2, 18...  0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Read in files\n",
    "intent_to_index = load_obj('../data/ag_news_char/intent_to_index')\n",
    "char_to_index = load_obj('../data/ag_news_char/char_to_index')\n",
    "train_df = pd.read_pickle('../data/ag_news_char/train.pickle')\n",
    "test_df = pd.read_pickle('../data/ag_news_char/test.pickle')\n",
    "\n",
    "# Feature/label lists\n",
    "X_train = train_df.X.tolist()\n",
    "Y_train = train_df.Y.tolist()\n",
    "X_test = test_df.X.tolist()\n",
    "Y_test = test_df.Y.tolist()\n",
    "\n",
    "print(\"\\nDictionary mapping characters to integer indices:\\n{}\\n\\nDictionary mapping labels to integer indices:\\n{}\".format(char_to_index, intent_to_index))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a custom bucketing iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import random\n",
    "import numpy as np\n",
    "from mxnet.io import DataIter, DataBatch, DataDesc\n",
    "from mxnet import ndarray\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class BucketUtteranceIter(DataIter):\n",
    "    \"\"\"\n",
    "    This iterator can handle variable length feature arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, utterances, intents, batch_size, buckets, data_pad=-1, label_pad=-1, data_name='utterance',\n",
    "                 label_name='intent', dtype='float32'):\n",
    "        \"\"\"\n",
    "        :param utterances: list of list of int\n",
    "        :param intents: list of int\n",
    "        \"\"\"\n",
    "        super(BucketUtteranceIter, self).__init__()\n",
    "        buckets.sort()\n",
    "\n",
    "        nslice = 0  # Keep track of how many utterances are sliced\n",
    "        self.utterances = [[] for _ in buckets]\n",
    "        self.intents = [[] for _ in buckets]\n",
    "        self.indices = [[] for _ in buckets]\n",
    "\n",
    "        for i, utt in enumerate(utterances):\n",
    "            # Find the index of the smallest bucket that is larger than the sentence length\n",
    "            buck_idx = bisect.bisect_left(buckets, len(utt))\n",
    "\n",
    "            # Slice utterances that are too long to the largest bucket size\n",
    "            if buck_idx == len(buckets):\n",
    "                buck_idx = buck_idx - 1\n",
    "                nslice += 1\n",
    "                utt = utt[:buckets[buck_idx]]\n",
    "\n",
    "            # Pad utterances that are too short for their bucket\n",
    "            buff = np.full((buckets[buck_idx]), data_pad, dtype=dtype)\n",
    "            buff[:len(utt)] = utt\n",
    "\n",
    "            # Add data/label to bucket\n",
    "            self.utterances[buck_idx].append(buff)\n",
    "            self.intents[buck_idx].append(intents[i])\n",
    "            self.indices[buck_idx].append(i)\n",
    "\n",
    "        # Convert to list of array\n",
    "        self.utterances = [np.asarray(i, dtype=dtype) for i in self.utterances]\n",
    "        self.intents = [np.asarray(i, dtype=dtype) for i in self.intents]\n",
    "        self.indices = [np.asarray(i, dtype=dtype) for i in self.indices]\n",
    "\n",
    "        print(\"\\nWarning, {0} utterances sliced to largest bucket size.\".format(nslice)) if nslice > 0 else None\n",
    "        print(\"Utterances per bucket: {}\\nBucket sizes: {}\".format([arr.shape[0] for arr in self.utterances], buckets))\n",
    "\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.dtype = dtype\n",
    "        self.data_pad = data_pad\n",
    "        self.label_pad = label_pad\n",
    "        self.default_bucket_key = max(buckets)\n",
    "        self.layout = 'NT'\n",
    "\n",
    "        self.provide_data = [DataDesc(name=self.data_name,\n",
    "                                      shape=(self.batch_size, self.default_bucket_key),\n",
    "                                      layout=self.layout)]\n",
    "        self.provide_label = [DataDesc(name=self.label_name,\n",
    "                                       shape=(self.batch_size, ),\n",
    "                                       layout=self.layout)]\n",
    "\n",
    "        # create empty list to store batch index values\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the iterator to the beginning of the data.\n",
    "        \"\"\"\n",
    "        self.curr_idx = 0\n",
    "        # shuffle data in each bucket\n",
    "        random.shuffle(self.idx)\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.indices[i], self.utterances[i], self.intents[i] = shuffle(self.indices[i],\n",
    "                                                                           self.utterances[i],\n",
    "                                                                           self.intents[i])\n",
    "        self.ndindex = []\n",
    "        self.ndsent = []\n",
    "        self.ndlabel = []\n",
    "\n",
    "        # append the lists with an array\n",
    "        for i, buck in enumerate(self.utterances):\n",
    "            self.ndindex.append(ndarray.array(self.indices[i], dtype=self.dtype))\n",
    "            self.ndsent.append(ndarray.array(self.utterances[i], dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(self.intents[i], dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Returns the next batch of data.\n",
    "        \"\"\"\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        # i = batches index, j = starting record\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        indices = self.ndindex[i][j:j + self.batch_size]\n",
    "        utterances = self.ndsent[i][j:j + self.batch_size]\n",
    "        intents = self.ndlabel[i][j:j + self.batch_size]\n",
    "\n",
    "        return DataBatch([utterances],\n",
    "                         [intents],\n",
    "                         pad=0,\n",
    "                         index=indices,\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[DataDesc(name=self.data_name, shape=utterances.shape, layout=self.layout)],\n",
    "                         provide_label=[DataDesc(name=self.label_name, shape=intents.shape, layout=self.layout)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group utterances into predefined buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterances per bucket: [4, 295, 10379, 98894, 9972, 456]\n",
      "Bucket sizes: [32, 64, 128, 256, 512, 1024]\n",
      "Utterances per bucket: [0, 21, 658, 6289, 601, 31]\n",
      "Bucket sizes: [32, 64, 128, 256, 512, 1024]\n",
      "\n",
      "Batch 0 Bucket size 256\n",
      "Data\n",
      " [\n",
      "[[11.  8.  4. ... -1. -1. -1.]\n",
      " [ 8.  6. 20. ... -1. -1. -1.]\n",
      " [ 3. 10.  1. ... -1. -1. -1.]\n",
      " ...\n",
      " [12.  2. 20. ... -1. -1. -1.]\n",
      " [ 0. 20.  2. ... -1. -1. -1.]\n",
      " [ 4.  3.  0. ... -1. -1. -1.]]\n",
      "<NDArray 12x256 @cpu(0)>] \n",
      "Label\n",
      " [\n",
      "[0. 1. 3. 2. 2. 1. 1. 0. 2. 0. 3. 1.]\n",
      "<NDArray 12 @cpu(0)>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=12\n",
    "\n",
    "train_iter = BucketUtteranceIter(X_train, Y_train, batch_size, buckets=[32,64,128,256,512,1024])\n",
    "test_iter = BucketUtteranceIter(X_test, Y_test, batch_size, buckets=[32,64,128,256,512, 1024])\n",
    "\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i < 1:\n",
    "        print(\"\\nBatch {} Bucket size {}\\nData\\n {} \\nLabel\\n {}\\n\".format(i, batch.bucket_key, batch.data, batch.label))\n",
    "train_iter.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network symbol/module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "\n",
    "def bucketed_module(train_iter, vocab_size, dropout,num_label, smooth_alpha, default_bucket_key, context):\n",
    "    \"\"\"\n",
    "    :param train_iter:\n",
    "    :param vocab_size:\n",
    "    :param dropout:\n",
    "    :param num_label:\n",
    "    :param smooth_alpha:\n",
    "    :param default_bucket_key:\n",
    "    :param context:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def sym_gen(seq_len):\n",
    "        \"\"\"\n",
    "        :param seq_len: bucket size\n",
    "        :return: symbol for neural network architecture\n",
    "        \"\"\"\n",
    "        def conv(data, num_filter, kernel=(1, 1), stride=(1, 1), pad=(0, 0), name=None, suffix=''):\n",
    "            conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad,\n",
    "                                      no_bias=True, name='%s%s_conv2d' % (name, suffix))\n",
    "            bn = mx.sym.BatchNorm(data=conv, name='%s%s_batchnorm' % (name, suffix), fix_gamma=True)\n",
    "            act = mx.sym.Activation(data=bn, act_type='relu', name='%s%s_relu' % (name, suffix))\n",
    "            return act\n",
    "\n",
    "        def conv_block(data, num_filter, name):\n",
    "            conv1 = conv(data, kernel=(1, 3), num_filter=num_filter, pad=(0, 1), name='conv1' + str(name))\n",
    "            conv2 = conv(conv1, kernel=(1, 3), num_filter=num_filter, pad=(0, 1), name='conv2' + str(name))\n",
    "            return conv2\n",
    "\n",
    "        X_shape = (train_iter.batch_size, seq_len)\n",
    "        Y_shape = (train_iter.batch_size, )\n",
    "        \n",
    "        print(\"\\nNetwork architecture for bucket size {}\\n\".format(seq_len))\n",
    "\n",
    "        data = mx.sym.Variable(name=\"utterance\")\n",
    "        softmax_label = mx.sym.Variable(name=\"intent\")\n",
    "        print(\"data_input: \", data.infer_shape(utterance=X_shape)[1][0])\n",
    "        print(\"label input: \", softmax_label.infer_shape(intent=Y_shape)[1][0])\n",
    "\n",
    "        # Embed each character to 16 channels\n",
    "        embedded_data = mx.sym.Embedding(data, input_dim=vocab_size, output_dim=16)\n",
    "        embedded_data = mx.sym.Reshape(mx.sym.transpose(embedded_data, axes=(0, 2, 1)), shape=(0, 0, 1, -1))\n",
    "        print(\"embed layer output shape: \", embedded_data.infer_shape(utterance=X_shape)[1][0])\n",
    "\n",
    "        # Temporal Convolutional Layer (without activation)\n",
    "        temp_conv = mx.sym.Convolution(embedded_data, kernel=(1, 3), num_filter=64, pad=(0, 1))\n",
    "        print(\"Temp conv output shape: \", temp_conv.infer_shape(utterance=X_shape)[1][0])\n",
    "\n",
    "        # Create convolutional blocks with pooling in-between\n",
    "        block = conv_block(temp_conv, num_filter=64, name='block1_1')\n",
    "        block = conv_block(block, num_filter=64, name='block1_2')\n",
    "        pool = mx.sym.Pooling(block, kernel=(1, 3), stride=(1, 2), pad=(0, 1), pool_type='max')\n",
    "        print(\"Block 1 output shape: {}\".format(pool.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        block = conv_block(pool, num_filter=128, name='block2_1')\n",
    "        block = conv_block(block, num_filter=128, name='block2_2')\n",
    "        pool = mx.sym.Pooling(block, kernel=(1, 3), stride=(1, 2), pad=(0, 1), pool_type='max')\n",
    "        print(\"Block 2 output shape: {}\".format(pool.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        block = conv_block(pool, num_filter=256, name='block3_1')\n",
    "        block = conv_block(block, num_filter=256, name='block3_2')\n",
    "        pool = mx.sym.Pooling(block, kernel=(1, 3), stride=(1, 2), pad=(0, 1), pool_type='max')\n",
    "        print(\"Block 3 output shape: {}\".format(pool.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        block = conv_block(pool, num_filter=512, name='block4_1')\n",
    "        block = conv_block(block, num_filter=512, name='block4_2')\n",
    "        print(\"Block 4 output shape: {}\".format(block.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        pool_k = seq_len // 8\n",
    "        print(\"{0} pool kernel size {1}, stride 1\".format('avg', pool_k))\n",
    "        block = mx.sym.flatten(mx.sym.Pooling(block, kernel=(1, pool_k), stride=(1, 1), pad=(0, 0), pool_type='avg'))\n",
    "        print(\"flattened pooling output shape: {}\".format(block.infer_shape(utterance=X_shape)[1][0]))\n",
    "        block = mx.sym.Dropout(block, p=dropout)\n",
    "        print(\"dropout layer output shape: {}\".format(block.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        output = mx.sym.FullyConnected(block, num_hidden=num_label, flatten=True, name='output')\n",
    "        sm = mx.sym.SoftmaxOutput(output, softmax_label, smooth_alpha)\n",
    "        print(\"softmax output shape: {}\".format(sm.infer_shape(utterance=X_shape)[1][0]))\n",
    "\n",
    "        return sm, ('utterance',), ('intent',)\n",
    "\n",
    "    return mx.mod.BucketingModule(sym_gen=sym_gen, default_bucket_key=default_bucket_key, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture for bucket size 1024\n",
      "\n",
      "data_input:  (12, 1024)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 1024)\n",
      "Temp conv output shape:  (12, 64, 1, 1024)\n",
      "Block 1 output shape: (12, 64, 1, 512)\n",
      "Block 2 output shape: (12, 128, 1, 256)\n",
      "Block 3 output shape: (12, 256, 1, 128)\n",
      "Block 4 output shape: (12, 512, 1, 128)\n",
      "avg pool kernel size 128, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n"
     ]
    }
   ],
   "source": [
    "module = bucketed_module(train_iter, \n",
    "                         vocab_size=150, \n",
    "                         dropout=0.02,\n",
    "                         num_label=4,\n",
    "                         smooth_alpha=0.004,\n",
    "                         default_bucket_key=train_iter.default_bucket_key, \n",
    "                         context=mx.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on varying input sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture for bucket size 1024\n",
      "\n",
      "data_input:  (12, 1024)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 1024)\n",
      "Temp conv output shape:  (12, 64, 1, 1024)\n",
      "Block 1 output shape: (12, 64, 1, 512)\n",
      "Block 2 output shape: (12, 128, 1, 256)\n",
      "Block 3 output shape: (12, 256, 1, 128)\n",
      "Block 4 output shape: (12, 512, 1, 128)\n",
      "avg pool kernel size 128, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n",
      "\n",
      "Network architecture for bucket size 256\n",
      "\n",
      "data_input:  (12, 256)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 256)\n",
      "Temp conv output shape:  (12, 64, 1, 256)\n",
      "Block 1 output shape: (12, 64, 1, 128)\n",
      "Block 2 output shape: (12, 128, 1, 64)\n",
      "Block 3 output shape: (12, 256, 1, 32)\n",
      "Block 4 output shape: (12, 512, 1, 32)\n",
      "avg pool kernel size 32, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n",
      "\n",
      "Network architecture for bucket size 512\n",
      "\n",
      "data_input:  (12, 512)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 512)\n",
      "Temp conv output shape:  (12, 64, 1, 512)\n",
      "Block 1 output shape: (12, 64, 1, 256)\n",
      "Block 2 output shape: (12, 128, 1, 128)\n",
      "Block 3 output shape: (12, 256, 1, 64)\n",
      "Block 4 output shape: (12, 512, 1, 64)\n",
      "avg pool kernel size 64, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n",
      "\n",
      "Network architecture for bucket size 128\n",
      "\n",
      "data_input:  (12, 128)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 128)\n",
      "Temp conv output shape:  (12, 64, 1, 128)\n",
      "Block 1 output shape: (12, 64, 1, 64)\n",
      "Block 2 output shape: (12, 128, 1, 32)\n",
      "Block 3 output shape: (12, 256, 1, 16)\n",
      "Block 4 output shape: (12, 512, 1, 16)\n",
      "avg pool kernel size 16, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n",
      "\n",
      "Network architecture for bucket size 64\n",
      "\n",
      "data_input:  (12, 64)\n",
      "label input:  (12,)\n",
      "embed layer output shape:  (12, 16, 1, 64)\n",
      "Temp conv output shape:  (12, 64, 1, 64)\n",
      "Block 1 output shape: (12, 64, 1, 32)\n",
      "Block 2 output shape: (12, 128, 1, 16)\n",
      "Block 3 output shape: (12, 256, 1, 8)\n",
      "Block 4 output shape: (12, 512, 1, 8)\n",
      "avg pool kernel size 8, stride 1\n",
      "flattened pooling output shape: (12, 512)\n",
      "dropout layer output shape: (12, 512)\n",
      "softmax output shape: (12, 4)\n"
     ]
    }
   ],
   "source": [
    "# Reduce learning rate every 3 epochs\n",
    "batches_per_epoch = int(sum([len(bucket) for bucket in train_iter.ndsent])/batch_size)\n",
    "step = 3 * batches_per_epoch\n",
    "schedule = mx.lr_scheduler.FactorScheduler(step=step, factor=0.97)\n",
    "\n",
    "# Initialize convolutional filter weights using MSRAPRelu to aid training deeper architectures\n",
    "init = mx.initializer.Mixed(patterns=['conv2d_weight', '.*'],\n",
    "                            initializers=[mx.initializer.MSRAPrelu(factor_type='avg', slope=0.25),\n",
    "                                          mx.initializer.Normal(sigma=0.02)])\n",
    "\n",
    "# Learn network weights from data\n",
    "module.fit(train_data=train_iter,\n",
    "           eval_data=test_iter,\n",
    "           eval_metric=mx.metric.Accuracy(),\n",
    "           optimizer='sgd',\n",
    "           optimizer_params={'learning_rate': 0.06,\n",
    "                             'momentum': 0.93,\n",
    "                             'lr_scheduler': schedule},\n",
    "           # initializer=init,\n",
    "           num_epoch=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
